{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6881568,"sourceType":"datasetVersion","datasetId":3953777}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-18T03:20:06.639173Z","iopub.execute_input":"2023-11-18T03:20:06.639920Z","iopub.status.idle":"2023-11-18T03:20:19.862130Z","shell.execute_reply.started":"2023-11-18T03:20:06.639883Z","shell.execute_reply":"2023-11-18T03:20:19.861103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-11-18T03:34:16.150579Z","iopub.execute_input":"2023-11-18T03:34:16.150957Z","iopub.status.idle":"2023-11-18T03:34:21.831952Z","shell.execute_reply.started":"2023-11-18T03:34:16.150932Z","shell.execute_reply":"2023-11-18T03:34:21.831068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import BertTokenizer\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# Custom function to split text into sentences\ndef split_into_sentences(text):\n    sentences = []\n    sentence = ''\n    for char in text:\n        sentence += char\n        if char in '.!?':\n            sentences.append(sentence.strip())\n            sentence = ''\n    if sentence:\n        sentences.append(sentence.strip())\n    return sentences\n\n# Function to chunk text by custom sentences while respecting the token limit\ndef chunk_text_by_sentences(text, tokenizer, max_length):\n    sentences = split_into_sentences(text)\n    current_chunk = \"\"\n    for sentence in sentences:\n        tokenized_sentence = tokenizer.encode(sentence, add_special_tokens=False)\n        if len(tokenizer.encode(current_chunk)) + len(tokenized_sentence) < max_length:\n            current_chunk += sentence + \" \"\n        else:\n            if current_chunk:\n                yield current_chunk\n            current_chunk = sentence + \" \"\n    if current_chunk:\n        yield current_chunk\n\n# Worker function for processing each row\ndef process_row(row, tokenizer, max_length):\n    text_chunks = list(chunk_text_by_sentences(row['text'], tokenizer, max_length))\n    return [{'text': chunk, 'score': row['score'], 'chunk_index': row.name} for chunk in text_chunks]\n\n# Preprocess function with multithreading\ndef preprocess_data(data, tokenizer, max_length, num_threads=10):\n    new_rows = []\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        # Create future tasks for each row\n        futures = [executor.submit(process_row, row, tokenizer, max_length) for _, row in data.iterrows()]\n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Preprocessing Data\"):\n            new_rows.extend(future.result())\n    return pd.DataFrame(new_rows)\n\n# Load datasets\ntrain_data = pd.read_csv('/kaggle/input/sahaj-stat/train.csv')\nvalidation_data = pd.read_csv('/kaggle/input/sahaj-stat/sample_submission.csv')\n\n# Initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\nmax_length = 512  # Token limit for BERT\n\n# Preprocess training and validation data\ntrain_data_processed = preprocess_data(train_data, tokenizer, max_length)\nvalidation_data_processed = preprocess_data(validation_data, tokenizer, max_length)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T04:03:13.912702Z","iopub.execute_input":"2023-11-18T04:03:13.913439Z","iopub.status.idle":"2023-11-18T04:12:23.363388Z","shell.execute_reply.started":"2023-11-18T04:03:13.913407Z","shell.execute_reply":"2023-11-18T04:12:23.362480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize the training data\ntrain_texts = train_data_processed['text'].tolist()\ntrain_scores = train_data_processed['score'].values\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\n\n# Convert scores to PyTorch tensors\ntrain_scores = torch.tensor(train_scores, dtype=torch.float32)\n\n# Tokenize the validation data\nval_texts = validation_data_processed['text'].tolist()\nval_scores = validation_data_processed['score'].values\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt')\n\n# Create DataLoaders for training and validation data\ntrain_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_scores)\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n\nval_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(val_scores, dtype=torch.float32))\nval_dataloader = DataLoader(val_dataset, batch_size=4)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T04:12:47.761239Z","iopub.execute_input":"2023-11-18T04:12:47.761589Z","iopub.status.idle":"2023-11-18T04:13:37.554361Z","shell.execute_reply.started":"2023-11-18T04:12:47.761562Z","shell.execute_reply":"2023-11-18T04:13:37.553491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Load BERT-Large for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=1)\nmodel.train()\n\n# Define loss function and optimizer with weight decay (L2 regularization)\ncriterion = nn.MSELoss()\nweight_decay = 0.1  # Adjust the weight decay hyperparameter as needed\noptimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=weight_decay)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_dataloader) * 5)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T04:13:41.486887Z","iopub.execute_input":"2023-11-18T04:13:41.487493Z","iopub.status.idle":"2023-11-18T04:13:51.052183Z","shell.execute_reply.started":"2023-11-18T04:13:41.487460Z","shell.execute_reply":"2023-11-18T04:13:51.051067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\nepochs = 3\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\n\nfor epoch in range(epochs):\n    model.train()\n    total_rmse = 0.0\n    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs}'):\n        optimizer.zero_grad()\n        input_ids, attention_mask, targets = batch\n        input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        loss = criterion(outputs.logits.view(-1), targets)\n        rmse = torch.sqrt(loss)\n        rmse.backward()\n        optimizer.step()\n        scheduler.step()\n        total_rmse += rmse.item()\n\n    # Calculate and print RMSE for this epoch\n    avg_rmse = total_rmse / len(train_dataloader)\n    print(f'Epoch {epoch+1}/{epochs}, RMSE: {avg_rmse:.4f}')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T04:13:51.054367Z","iopub.execute_input":"2023-11-18T04:13:51.055088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained('/kaggle/working/bert_large_regression_model')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import BertForSequenceClassification, BertTokenizer\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Load the submission data from sample_submission.csv\nsubmission_data = pd.read_csv('/kaggle/input/sahaj-stat/sample_submission.csv')\n\n# Initialize the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\n# Tokenize the submission data\nsubmission_encodings = tokenizer(submission_data['text'].tolist(), truncation=True, padding=True, return_tensors='pt')\n\n# Create a DataLoader for submission data\nsubmission_dataset = TensorDataset(submission_encodings['input_ids'], submission_encodings['attention_mask'])\nsubmission_dataloader = DataLoader(submission_dataset, batch_size=4)\n\n# Load the saved model (replace 'bert_large_regression_model' with your model's actual path)\nmodel = BertForSequenceClassification.from_pretrained('/kaggle/working/bert_large_regression_model')\n\n# Use GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\n\n# Make predictions on the submission data\nmodel.eval()  # Set the model to evaluation mode for inference\n\n# Create a list to store the predicted scores\npredicted_scores = []\n\n# Iterate over the submission data with tqdm progress bar\nfor batch in tqdm(submission_dataloader, desc=\"Inferencing\"):\n    with torch.no_grad():\n        input_ids, attention_mask = batch\n        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predicted_scores.extend(outputs.logits.view(-1).cpu().numpy())\n\n# Assuming the preprocessed submission data with 'chunk_index' is available as preprocessed_submission_data\n# Add predicted scores to the preprocessed data\npreprocessed_submission_data = pd.read_csv('/path/to/your/preprocessed_submission_data.csv')  # Load your preprocessed data\npreprocessed_submission_data['predicted_score'] = predicted_scores\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T08:34:26.626149Z","iopub.execute_input":"2023-11-08T08:34:26.626516Z","iopub.status.idle":"2023-11-08T08:35:08.444067Z","shell.execute_reply.started":"2023-11-08T08:34:26.626486Z","shell.execute_reply":"2023-11-08T08:35:08.443147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to concatenate texts and average scores\ndef concatenate_and_average(data):\n    concatenated_texts = []\n    averaged_scores = []\n    grouped_data = data.groupby('chunk_index')\n\n    for _, group in grouped_data:\n        concatenated_text = ' '.join(group['text'])\n        average_score = group['predicted_score'].mean()\n        concatenated_texts.append(concatenated_text)\n        averaged_scores.append(average_score)\n\n    return pd.DataFrame({'text': concatenated_texts, 'average_score': averaged_scores})\n\n# Apply the function to the preprocessed submission data\nfinal_results = concatenate_and_average(preprocessed_submission_data)\n\n# final_results now contains the concatenated texts and their average scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the updated submission data to a new CSV file\nfinal_results.to_csv('/kaggle/working/predicted_submission_512_token.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T08:35:10.693441Z","iopub.execute_input":"2023-11-08T08:35:10.694302Z","iopub.status.idle":"2023-11-08T08:35:10.744326Z","shell.execute_reply.started":"2023-11-08T08:35:10.694268Z","shell.execute_reply":"2023-11-08T08:35:10.743608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data","metadata":{"execution":{"iopub.status.busy":"2023-11-08T08:35:11.719094Z","iopub.execute_input":"2023-11-08T08:35:11.719775Z","iopub.status.idle":"2023-11-08T08:35:11.732117Z","shell.execute_reply.started":"2023-11-08T08:35:11.719741Z","shell.execute_reply":"2023-11-08T08:35:11.731210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_data.score.max()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T08:35:12.141082Z","iopub.execute_input":"2023-11-08T08:35:12.141423Z","iopub.status.idle":"2023-11-08T08:35:12.148155Z","shell.execute_reply.started":"2023-11-08T08:35:12.141396Z","shell.execute_reply":"2023-11-08T08:35:12.147196Z"},"trusted":true},"execution_count":null,"outputs":[]}]}